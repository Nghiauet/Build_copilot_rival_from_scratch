{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 tokens for \"sex\": ['', 's', 'ex']\n",
      "CamemBERT tokens for \"being\": ['be', 'ing']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer \n",
    "def tok_list(tokenizer , text ):\n",
    "    input_ids = tokenizer(text , add_special_tokens = False)[\"input_ids\"]\n",
    "    return [tokenizer.decode(tok) for tok in input_ids] \n",
    "tokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "tokenzier_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "print(f'T5 tokens for \"sex\": {tok_list(tokenizer_T5,\"sex\")}')\n",
    "print(f'CamemBERT tokens for \"being\": {tok_list(tokenzier_camembert,\"being\")}')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tokenzier model and measuring tokenzier performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "python_code = r\"\"\"def say_hello():\n",
    " print(\"Hello, World!\")\n",
    "# Print it\n",
    "say_hello()\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer(python_code).tokens())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():', (13, 16)), ('Ċ', (16, 17)), ('Ġprint', (17, 23)), ('(\"', (23, 25)), ('Hello', (25, 30)), (',', (30, 31)), ('ĠWorld', (31, 37)), ('!\")', (37, 40)), ('Ċ', (40, 41)), ('#', (41, 42)), ('ĠPrint', (42, 48)), ('Ġit', (48, 51)), ('Ċ', (51, 52)), ('say', (52, 55)), ('_', (55, 56)), ('hello', (56, 61)), ('()', (61, 63)), ('Ċ', (63, 64))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`a` is encoded as `b'a'` with a single byte: 97\n",
      "`€` is encoded as `b'\\xe2\\x82\\xac'` with three bytes: [226, 130, 172]\n"
     ]
    }
   ],
   "source": [
    "a, e = u\"a\", u\"€\"\n",
    "byte = ord(a.encode(\"utf-8\"))\n",
    "print(f'`{a}` is encoded as `{a.encode(\"utf-8\")}` with a single byte: {byte}')\n",
    "byte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\n",
    "print(f'`{e}` is encoded as `{e.encode(\"utf-8\")}` with three bytes: {byte}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our base vocabulary: 256\n",
      "First element: `!`, last element: `Ń`\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "print(f'Size of our base vocabulary: {len(base_vocab)}')\n",
    "print(f'First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '¡',\n",
       " '¢',\n",
       " '£',\n",
       " '¤',\n",
       " '¥',\n",
       " '¦',\n",
       " '§',\n",
       " '¨',\n",
       " '©',\n",
       " 'ª',\n",
       " '«',\n",
       " '¬',\n",
       " '®',\n",
       " '¯',\n",
       " '°',\n",
       " '±',\n",
       " '²',\n",
       " '³',\n",
       " '´',\n",
       " 'µ',\n",
       " '¶',\n",
       " '·',\n",
       " '¸',\n",
       " '¹',\n",
       " 'º',\n",
       " '»',\n",
       " '¼',\n",
       " '½',\n",
       " '¾',\n",
       " '¿',\n",
       " 'À',\n",
       " 'Á',\n",
       " 'Â',\n",
       " 'Ã',\n",
       " 'Ä',\n",
       " 'Å',\n",
       " 'Æ',\n",
       " 'Ç',\n",
       " 'È',\n",
       " 'É',\n",
       " 'Ê',\n",
       " 'Ë',\n",
       " 'Ì',\n",
       " 'Í',\n",
       " 'Î',\n",
       " 'Ï',\n",
       " 'Ð',\n",
       " 'Ñ',\n",
       " 'Ò',\n",
       " 'Ó',\n",
       " 'Ô',\n",
       " 'Õ',\n",
       " 'Ö',\n",
       " '×',\n",
       " 'Ø',\n",
       " 'Ù',\n",
       " 'Ú',\n",
       " 'Û',\n",
       " 'Ü',\n",
       " 'Ý',\n",
       " 'Þ',\n",
       " 'ß',\n",
       " 'à',\n",
       " 'á',\n",
       " 'â',\n",
       " 'ã',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'æ',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'ë',\n",
       " 'ì',\n",
       " 'í',\n",
       " 'î',\n",
       " 'ï',\n",
       " 'ð',\n",
       " 'ñ',\n",
       " 'ò',\n",
       " 'ó',\n",
       " 'ô',\n",
       " 'õ',\n",
       " 'ö',\n",
       " '÷',\n",
       " 'ø',\n",
       " 'ù',\n",
       " 'ú',\n",
       " 'û',\n",
       " 'ü',\n",
       " 'ý',\n",
       " 'þ',\n",
       " 'ÿ',\n",
       " 'Ā',\n",
       " 'ā',\n",
       " 'Ă',\n",
       " 'ă',\n",
       " 'Ą',\n",
       " 'ą',\n",
       " 'Ć',\n",
       " 'ć',\n",
       " 'Ĉ',\n",
       " 'ĉ',\n",
       " 'Ċ',\n",
       " 'ċ',\n",
       " 'Č',\n",
       " 'č',\n",
       " 'Ď',\n",
       " 'ď',\n",
       " 'Đ',\n",
       " 'đ',\n",
       " 'Ē',\n",
       " 'ē',\n",
       " 'Ĕ',\n",
       " 'ĕ',\n",
       " 'Ė',\n",
       " 'ė',\n",
       " 'Ę',\n",
       " 'ę',\n",
       " 'Ě',\n",
       " 'ě',\n",
       " 'Ĝ',\n",
       " 'ĝ',\n",
       " 'Ğ',\n",
       " 'ğ',\n",
       " 'Ġ',\n",
       " 'ġ',\n",
       " 'Ģ',\n",
       " 'ģ',\n",
       " 'Ĥ',\n",
       " 'ĥ',\n",
       " 'Ħ',\n",
       " 'ħ',\n",
       " 'Ĩ',\n",
       " 'ĩ',\n",
       " 'Ī',\n",
       " 'ī',\n",
       " 'Ĭ',\n",
       " 'ĭ',\n",
       " 'Į',\n",
       " 'į',\n",
       " 'İ',\n",
       " 'ı',\n",
       " 'Ĳ',\n",
       " 'ĳ',\n",
       " 'Ĵ',\n",
       " 'ĵ',\n",
       " 'Ķ',\n",
       " 'ķ',\n",
       " 'ĸ',\n",
       " 'Ĺ',\n",
       " 'ĺ',\n",
       " 'Ļ',\n",
       " 'ļ',\n",
       " 'Ľ',\n",
       " 'ľ',\n",
       " 'Ŀ',\n",
       " 'ŀ',\n",
       " 'Ł',\n",
       " 'ł',\n",
       " 'Ń']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():', (13, 16)), ('Ċ', (16, 17)), ('Ġprint', (17, 23)), ('(\"', (23, 25)), ('Hello', (25, 30)), (',', (30, 31)), ('ĠWorld', (31, 37)), ('!\")', (37, 40)), ('Ċ', (40, 41)), ('#', (41, 42)), ('ĠPrint', (42, 48)), ('Ġit', (48, 51)), ('Ċ', (51, 52)), ('say', (52, 55)), ('_', (55, 56)), ('hello', (56, 61)), ('()', (61, 63)), ('Ċ', (63, 64))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of the vocabulary: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(python_code).tokens())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ', 35496), ('Ġ=================================================================', 38093), ('Ġ----------------------------------------------------------------', 16529), ('================================================================', 23926), ('................................................................', 23193)]\n",
      "['t: ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', 't: Ġ=================================================================  =================================================================', 't: Ġ----------------------------------------------------------------  ----------------------------------------------------------------', 't: ================================================================ ================================================================', 't: ................................................................ ................................................................', 't: ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', 't: ________________________________________________________________ ________________________________________________________________', 't: ---------------------------------------------------------------- ----------------------------------------------------------------']\n"
     ]
    }
   ],
   "source": [
    "# extract the longest work token\n",
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "print(tokens[:5])\n",
    "print([f't: {t} {tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[:8]]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ' gazed', ' informants', ' Collider', ' regress', 'ominated', ' amplification', 'Compar', '….\"', ' (/', 'Commission', ' Hitman']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[:12]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "from datasets import load_dataset\n",
    "length = 100000\n",
    "dataset_name = 'transformerbook/codeparrot-train'\n",
    "dataset = load_dataset(dataset_name, split = 'train' , streaming = True)\n",
    "iter_dataset = iter(dataset) \n",
    "\n",
    "def batch_iterator(batch_size = 10 ):\n",
    "    for _ in tqdm(range(0 , length, batch_size)):\n",
    "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
    "new_tokenizer = AutoTokenizer.train_new_from_iterator(batch_iterator(),\n",
    "                                                      vocab_size = 12500 , \n",
    "                                                      initial_alphabet = base_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEC_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
